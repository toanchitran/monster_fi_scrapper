{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAN CHI TOAN - toanchitran2302@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today().isoformat()\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re \n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "def monster (job_query = None, location_query=''):\n",
    "    \n",
    "    \n",
    "    if job_query is None:\n",
    "        job_query = 'data-analytics'\n",
    "    else:\n",
    "        job_query = job_query.split()\n",
    "        job_query= '-'.join(w for w in job_query)\n",
    "    \n",
    "    if location_query is not None:\n",
    "        location_query = location_query.split()\n",
    "        location_query = '-'.join(w for w in location_query)\n",
    "        \n",
    "        \n",
    "    #Get job detail job description fuction\n",
    "    def job(url):\n",
    "        job_page = requests.get(url)\n",
    "        job_soup = BeautifulSoup(job_page.text, 'lxml')\n",
    "        job_desc = job_soup.find('div', attrs={'id':'JobDescription'}).text\n",
    "        return job_desc\n",
    "\n",
    "    # Clean text\n",
    "    def clean_text(text):\n",
    "        lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "        \n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "        def chunk_space(chunk):\n",
    "            chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "            return chunk_out  \n",
    "        \n",
    "    \n",
    "        text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "        \n",
    "        # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "        try:\n",
    "            text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "        except:                                                            # in a way that this works, can occasionally throw\n",
    "            return                                                         # an exception\n",
    "        return text\n",
    "\n",
    "\n",
    "\n",
    "    # Scrapt the first page to find the number of jobs \n",
    "    base_URL= ''.join(['https://www.monster.com/jobs/search/?q='+job_query+'&where='+location_query])\n",
    "    base_page = requests.get(base_URL)\n",
    "    base_soup = BeautifulSoup(base_page.text, 'html.parser')\n",
    "    number_job = sum(int(i) for i in re.findall('\\d+', base_soup.find('h2', attrs={'class':'figure'}).text))\n",
    "\n",
    "    page_num = int(number_job/20)\n",
    "\n",
    "    #Scrapt all the job\n",
    "    crawl_page = ''.join([base_URL, '&page=', str(page_num)])\n",
    "    page = requests.get(crawl_page)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    # We need those dataframe to store information on company, job title, link, location and job description\n",
    "    company = pd.DataFrame(columns=['Company'])\n",
    "    location = pd.DataFrame(columns=['Location'])\n",
    "    title = pd.DataFrame(columns=['Title'])\n",
    "    URL = pd.DataFrame(columns=['URL'])\n",
    "    job_description = pd.DataFrame(columns=['Job Description'])\n",
    "    final_query= pd.DataFrame(columns=['Job Query'])\n",
    "    links = []\n",
    "    \n",
    "    #Append Job Query\n",
    "    \n",
    "\n",
    "    # Find companies\n",
    "    companies = soup.findAll(name='div', attrs={'class':'company'})\n",
    "\n",
    "    # Find job titles\n",
    "    titles = soup.findAll(name='h2', attrs={'class':'title'})\n",
    "\n",
    "    # Find job locations\n",
    "    locations = soup.findAll(name='div', attrs={'class':'location'})\n",
    "\n",
    "    # Append all companies into companies dataframe\n",
    "    for a in companies:\n",
    "        comp=a.text.strip()\n",
    "        company= company.append({'Company':comp},ignore_index=True)\n",
    "\n",
    "    # Append all job locations to locations dataframe\n",
    "    for b in locations:\n",
    "        loc = b.text.strip()\n",
    "        if loc != 'Location\\n\\nx':\n",
    "            location= location.append({'Location':loc}, ignore_index=True)\n",
    "\n",
    "\n",
    "    for c in titles:\n",
    "        link = c.find('a').attrs['href']\n",
    "        links.append(link)\n",
    "        title = title.append({'Title': c.text.strip()}, ignore_index=True)\n",
    "        URL=URL.append({'URL':link}, ignore_index=True)\n",
    "        final_query = final_query.append({'Job Query':job_query}, ignore_index=True)\n",
    "   \n",
    "    for e in links:\n",
    "        job_desc_l = job(e)\n",
    "        job_clean = clean_text(job_desc_l)\n",
    "        job_clean = job_clean.replace(\"#jobBodyContent ul { \t\tmargin-bottom: 13px; \t\tline-height: 16px; \t}\", \"\")\n",
    "        job_clean = job_clean.replace(\"#ejb_SubHdTxt, .ejb_SubHdTxt { background-color: #f1f1f1;} \t\t.addthis_button {font-size:8pt;} div#content{ font-size: 12px; /*font-family: Arial, Helvetica, sans-serif; */ \t\t\tline-height: 20px; \t\t\t/*width: 758px; */ \t\t\toverflow: hidden; \t\t} \t\timg#barona_top_banner{ \t\t\twidth: 100%; \t\t\theight: auto; \t\t\tmargin-bottom: 30px; \t\t}\t \t\tdiv#content_text{ \t\t\tmargin-left: 40px; \t\t\tmargin-right: 40px; \t\t} \t\tdiv#barona-text-area-padding{ \t\t\tpadding-bottom: 40px; \t\t} \t\tdiv#content_text a{font-size: 14px; font-weight: bold; color: #0d51b4; text-decoration: none;} div#content_text h1{font-size: 26px; font-weight: bold; margin-bottom: 30px;} \t \t\tdiv#content_text h2{font-size: 16px; font-weight: bold; } \t\tdiv#content_text span.company-title{font-weight: bold; } div#employer-company-card{ \t\t\tborder-top: 1px solid #dedede; \t\t\tpadding-top: 40px; \t\t\tpadding-bottom: 40px;\t\t \t\t} \t\timg#barona-employer-logo{ \t\t\tmax-width: 185px; \t\t\theight: auto; \t\t\tmargin-bottom: 20px; \t\t}\t \t\tdiv#barona-company-card{ \t\t\tborder-top: 1px solid #dedede; \t\t\tpadding-top: 40px; \t\t\tpadding-bottom: 40px;\t\t \t\t} \t\timg#barona-logo{ \t\t\tmax-width: 185px; \t\t\theight: auto; \t\t\tmargin-bottom: 20px; \t\t} \", \"\")\n",
    "        job_description = job_description.append({'Job Description':job_clean}, ignore_index=True)\n",
    "        sleep(1)\n",
    "\n",
    "    job=pd.concat([final_query, company, location, title, URL, job_description], axis=1)\n",
    "    \n",
    "    # Uncomment line below to create a csv file with the new job found\n",
    "    #job.to_csv('monster_fi_'+today+'_'+job_query''_'+location_query+'.csv', encoding='utf-8')   \n",
    "    \n",
    "    return job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
